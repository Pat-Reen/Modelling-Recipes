---
title: "Modelling recipes"
knit: (function(input_file, encoding) {out_dir <- 'docs';rmarkdown::render(input_file,encoding=encoding, output_file=file.path(dirname(input_file), out_dir,'RecipeBook.html'))})
subtitle: "With insurance data"
author: "Pat Reen"
output: 
  rmdformats::downcute:
    self_contained: true
    code_folding: hide
---
# Overview {.tabset .tabset-fade .tabset-pills}
## Background 
This document sets out a few practical recipes for modeling with insurance data, covering

* Common data transforms, summary stats, and simple visualisation
* Regression 
  + Grouped vs ungrouped data
  + Choice of: response distribution, link (and offsets), explanatory variables 
  + Modeling variance to industry/ reference (A/E or A - E)
  + Model selection - stepwise regression, likelihood tests
  + Predictions, confidence intervals and visualisations

## Libraries
A list of packages used in the recipe book.

```{r Setup, class.source = 'fold-show', results='hide', message=FALSE, warning=FALSE}
library(rmdformats) # theme for the HTML doc
library(kableExtra) # formatting tables
library(scales)     # data formatting  
library(ggplot2)    # graphs
library(GGally)     # extension of ggplot2
library(dplyr)      # data manipulation
library(corrplot)   # correlation matrix visualisation
```

# Data Simulation {.tabset .tabset-fade .tabset-pills}
This section sets out a method for generating dummy data. The simulated data is intended to reflect typical data used in an analysis of disability income incidence experience and is used throughout this analysis. Replace this data with your actual data.

## Simulating policies
Here we are using simulated data to fit a model. Replace with own data. We start by simulating a mix of 200k policies over 3 years. Some simplifying assumptions e.g. nil lapse/ new bus, no indexation.

```{r Generate dataframe}
# set the seed value (for the random number generator) so that the simulated data frame can be replicated later
set.seed(10)
# create 200k policies
n <- 200000

# data frame columns
# mix of business for benefit_period, waiting_period, occupation taken from [preso] 
# policy_year skewed to early years, but tail is fat
df <- data.frame(id = c(1:n), cal_year = 2018,policy_year = round(rweibull(n, scale=5, shape=2),0)) 
df <- df %>% mutate(sex = replicate(n,sample(c("m","f","u"), size=1, replace=TRUE, prob=c(.75,.20,.05))),
smoker = replicate(n,sample(c("n","s","u"), size=1, replace=TRUE, prob=c(.85,.1,.05))),
benefit_period = replicate(n,sample(c("a65","2yr","5yr"), size=1, replace=TRUE, prob=c(.76,.12,.12))),
waiting_period = replicate(n,sample(c("14d","30d","90d","720d"), size=1, replace=TRUE, prob=c(.04,.7,.15,.11))),
occupation = replicate(n,sample(c("prof","sed","techn","blue","white"), size=1, replace=TRUE, prob=c(.4,.2,.2,.1,.1))),
# age and policy year correlated; age normally distributed around 40 + policy_year (where policy_year is distributed around 5 years), floored at 25, capped at 60
age = round(pmax(pmin(rnorm(n,mean = 40+policy_year, sd = 5),60),25),0),
# sum_assured and age correlated; sum assured normally distributed around some mean (dependent on age rounded to 10), floored at 500
sum_assured = round(pmax(rnorm(n,mean = round(age,-1)*100+1000, sd = 2000),500),0))
# generate 3 years of exposure for the 200k policies => assume no lapses or new business
df2 <- df %>% mutate(cal_year=cal_year+1,policy_year=policy_year+1,age=age+1)
df3 <- df2 %>% mutate(cal_year=cal_year+1,policy_year=policy_year+1,age=age+1)
df <- rbind(df,df2,df3)

```

## Expected claim rate 
Set p values from which to simulate claims. The crude p values below were derived from the Society of Actuaries Analysis of USA Individual Disability Claim Incidence Experience from 2006 to 2014.

```{r Expected claim rate}

# by cause, age and sex, based upon polynomials fitted to crude actual rates
# sickness
f_sick_age_m <- function(age) {-0.0000003*age^3 + 0.000047*age^2 - 0.00203*age + 0.02715}
f_sick_age_f <- function(age) {-0.0000002*age^3 + 0.000026*age^2 - 0.00107*age + 0.01550} 	  	 	  
f_sick_age_u <- function(age) {f_sick_age_f(age)*1.2}
f_sick_age   <- function(age,sex) {case_when(sex == "m" ~ f_sick_age_m(age), sex == "f" ~ f_sick_age_f(age), sex == "u" ~ f_sick_age_u(age))}

# accident
f_acc_age_m <- function(age) {-0.00000002*age^3 + 0.000004*age^2 - 0.00020*age + 0.00340}
f_acc_age_f <- function(age) {-0.00000004*age^3 + 0.000007*age^2 - 0.00027*age + 0.00374} 	  	 	  
f_acc_age_u <- function(age) {f_sick_age_f(age)*1.2}
f_acc_age   <- function(age,sex) {case_when(sex == "m" ~ f_acc_age_m(age), sex == "f" ~ f_acc_age_f(age), sex == "u" ~ f_acc_age_u(age))}

# smoker, wp and occ based upon ratio of crude actual rates by category
# occupation adjustment informed by FSC commentary on DI incidence experience
f_smoker   <- function(smoker) {case_when(smoker == "n" ~ 1, smoker == "s" ~ 1.45, smoker == "u" ~ 0.9)}
f_wp   <- function(waiting_period) {case_when(waiting_period == "14d" ~ 1.4, waiting_period == "30d" ~ 1, waiting_period == "90d" ~ 0.3, waiting_period == "720d" ~ 0.2)}
f_occ_sick   <- function(occupation) {case_when(occupation == "prof" ~ 1, occupation == "sed" ~ 1, occupation == "techn" ~ 1, occupation == "blue" ~ 1, occupation == "white" ~ 1)}
f_occ_acc   <- function(occupation) {case_when(occupation == "prof" ~ 1, occupation == "sed" ~ 1, occupation == "techn" ~ 4.5, occupation == "blue" ~ 4.5, occupation == "white" ~ 1)}

# anecdotal allowance for higher rates at larger policy size and for older policies
f_sa_sick <- function(sum_assured) {case_when(sum_assured<=6000 ~ 1, sum_assured>6000 & sum_assured<=10000 ~ 1.1, sum_assured>10000 ~ 1.3)}
f_sa_acc <- function(sum_assured) {case_when(sum_assured<=6000 ~ 1, sum_assured>6000 & sum_assured<=10000 ~ 1, sum_assured>10000 ~ 1)}
f_pol_yr_sick <- function(policy_year) {case_when(policy_year<=5 ~ 1, policy_year>5 & policy_year<=10 ~ 1.1, policy_year>10 ~ 1.3)}
f_pol_yr_acc <- function(policy_year) {case_when(policy_year<=5 ~ 1, policy_year>5 & policy_year<=10 ~ 1, policy_year>10 ~ 1)}
```

## Expected claims
Add the crude p values to the data and simulate 1 draw from a binomial with prob = p for each record. Gives us a vector of claim/no-claim for each policy. Some simplifying assumptions like independence of sample across years for each policy and independence of accident and sickness incidences.

```{r Expected claims}
# add crude expected
df$inc_sick_expected=f_sick_age(df$age,df$sex)*f_smoker(df$smoker)*f_wp(df$waiting_period)*f_occ_sick(df$occupation)*f_sa_sick(df$sum_assured)*f_pol_yr_sick(df$policy_year)
df$inc_acc_expected=f_acc_age(df$age,df$sex)*f_smoker(df$smoker)*f_wp(df$waiting_period)*f_occ_acc(df$occupation)*f_sa_acc(df$sum_assured)*f_pol_yr_acc(df$policy_year)
# add prediction
df$inc_count_sick = sapply(df$inc_sick_expected,function(z){rbinom(1,1,z)})
df$inc_count_acc = sapply(df$inc_acc_expected,function(z){rbinom(1,1,z)})*(1-df$inc_count_sick)
df$inc_count_tot = df$inc_count_sick + df$inc_count_acc
```

# Data Exploration {.tabset .tabset-fade .tabset-pills}

The sections below rely heavily upon the dplyr package. 

## Data structure
Looking at the metadata for the data frame and a sample of the contents.

```{r Data structure, class.source = 'fold-show'}
# glimpse() or str() returns detail on the structure of the data frame. Our data consists of 600k rows and 15 columns. The columns are policy ID, several explanatory variables like sex and smoker, expected counts of claim (inc_sick_expected and inc_acc_expected) and actual counts of claim (inc_count_sick/acc/tot).
glimpse(df)

# head() returns the first 6 rows of the data frame. Similar to head(), sample_n() returns rows from our data frame, however these are chosen randomly. e.g. sample_n(df,5,replace=FALSE)
head(df) %>% kbl(caption = "Sample of data")

# class() returns the class of a column.
class(df$benefit_period)
```

## Factors
From the above you'll note that the categorical columns are stored as characters. Factorising these makes them easier to work with in our models e.g. for BP factorise a65|2yr|5yr as 1|2|3. Factors are stored as integers and have labels that tell us what they are, they can be ordered and are useful for statistical analysis. 

```{r Factorise columns, class.source = 'fold-show'}
# table() returns a table of counts at each combination of column values. prop.table() converts these to a proportion. For example, applying this to the column "sex" shows us that ~75% of our data is "m" and that the other data are either "f" or "u" (unknown).
table(df$sex)
prop.table(table(df$sex))

# We can then convert the columns to factors based upon the values of the column and ordering by frequency.
df$sex <- factor(df$sex, levels = c("m","f","u"))
df$smoker <- factor(df$smoker, levels = c("n","s","u"))
df$benefit_period <- factor(df$benefit_period, levels = c("a65","2yr","5yr"))
df$waiting_period <- factor(df$waiting_period, labels = c("30d","14d","720d","90d"))
df$occupation <- factor(df$occupation, labels = c("prof", "sed","techn","white","blue"))
```

## Selection methods
table() is a method of summarizing data, returning a count at each combination of values in a column. sample() and sample_n() are other examples of selection methods. This section (not exhaustive) looks at a few more selection methods in dplyr.

```{r Selection Methods, class.source = 'fold-show', eval=FALSE}
# data subsets:  e.g. select from df where age <25 or >60 
subset(df, age <25 | age > 60) 
#	dropping columns:
		# exclude columns
		mycols <- names(df) %in% c("cal_year", "smoker")
		new_df <- df[!mycols]
		# exclude 3rd and 5th column
		new_df <- df[c(-3,-5)]
		# delete columns v1 and v2 from new_df
		new_df$v3 <- new_df$v5 <- NULL
#	keeping columns: 
		# select variables v1, v2, v3
		mycols <- names(df) %in% c("cal_year", "smoker")
		new_df <- df[!mycols]
		# select 1st and 5th to 7th variables
    new_df <- df[c(1,5:7)]
```

## Manipulation methods
We might want to modify our data frame to prepare it for fitting our models. The section below looks at a few simple data manipulations. Here we also introduce the infix operator (%>%); this operator passes the argument to the left of it over to the code on the right, so df %>% "operation" passes the data frame df over to the operation on the right.

```{r Manipulation Methods, class.source = 'fold-show', eval=FALSE}
# create a copy of the dataframe to work from
  new_df <- df
# simple manipulations
	# select as in the selection methods section, but using infix
	new_df %>% select(id, age) # or a range using select(1:5) or select(contains("sick")) or select(starts_with("inc")); others e.g. ends_with(), last_col(), select(-age)
	# replace values in a column
	replace(new_df$sex,new_df$sex=="u","m") # no infix in base r
	# Rename, id to pol_id
	new_df %>% rename(pol_id = id)  #or (reversing the renaming)
	new_df %>% select(pol_id = id)  
	# alter data
	new_df <- new_df %>% mutate(inc_tot_expected = inc_acc_expected + inc_sick_expected) # need to assign the output back to the data frame
	# transmute - select and mutate simultaneously 
	new_df2 <- new_df %>% transmute(id, age, birth_year = cal_year - age)
	# sort
	new_df %>% arrange(desc(age))
	# filter
	new_df %>% filter(benefit_period == "a65", age <65) # or
	new_df %>% filter(benefit_period %in% c("a65","5yr"))
# aggregations
	# group by, also ungroup()
	new_df %>% group_by(sex) %>% # can add a mutate to group by which will aggregate only to the level specified in the group_by e.g. 
	mutate(sa_by_sex = sum(sum_assured)) # adds a new column with the total sum assured by sex.
	# after doing this, ungroup() in order to apply future operations to all records individually
	# count, sorting by most frequent and weighting by another column
	new_df %>% count(sex, wt= sum_assured, sort=TRUE)  # counts the number of entries for each value of sex, weighted by sum assured
	# summarize takes many observations and turns them into one observation. mean(), median(), min(), max(), and n() for the size of the group
	new_df %>% summarize(total = sum(sum_assured), min_age = min(age), max_age = max(age), max(inc_tot_expected)) 
	new_df %>% group_by(sex) %>% summarise(n = n())
	table(new_df$sex) # returns count by sex; no infix in base r
# outliers
	new_df %>% top_n(10, inc_tot_expected) # also operates on grouped table - returns top n per group
# window functions
	# lag - offset vector by 1 e.g. v <- c(1,3,6,14); so - lag(v) = NA 1 3 6
	new_df %>% arrange(id,age) %>% mutate(ifelse(id==lag(id),age - lag(age),1))
```

## Missing data
By default, the regression model will exclude any observation with missing values on its predictors. Missing values can be treated as a seperate category for categorical data. For missing numeric data, imputation is a potential solution.

```{r Missing data, class.source = 'fold-show', eval=FALSE}
# find the average age among non-missing values
summary(df$age)
# impute missing age values with the mean age
df$imputed_age <- ifelse(is.na(df$age)==TRUE,round(mean(df$age, na.rm=TRUE),2),df$age)
# create missing value indicator for age
df$missing_age <- ifelse(is.na(df$age)==TRUE,1,0)
```

## Review exposure data 
The tables and graphs that follow look at:

* the mix of business over rating factors using some of the selection methods described: These are all consistent with the simulation specification. 
* the correlation of ordered numerical rating factors: age and sum assured as well as age and policy year are positively correlated.

```{r Review exposure data, class.source = 'fold-show', warning = FALSE}
# look at distribution by single rating factors
df %>% count(benefit_period, wt = NULL, sort = TRUE) %>% mutate(freq = percent(round(n / sum(n),2))) %>% format(n, big.mark=",") %>% kbl(caption = "Benefit period mix")
df %>% count(waiting_period, wt = NULL, sort = TRUE) %>% mutate(freq = percent(round(n / sum(n),2))) %>% format(n, big.mark=",")  %>% kbl(caption = "Waiting period mix")
df %>% count(occupation, wt = NULL, sort = TRUE) %>% mutate(freq = percent(round(n / sum(n),2))) %>% format(n, big.mark=",") %>% kbl(caption = "Occupation mix")

# consider a histogram to show the distribution of numeric data
hist(df$age, main = "Histogram of age", xlab = "Age", ylab = "Frequency")
hist(df$sum_assured, main = "Histogram of sum assured", xlab = "Sum assured", ylab = "Frequency")
hist(df$policy_year, main = "Histogram of policy year", xlab = "Policy year", ylab = "Frequency")

# correlation of ordered numeric explanatory variables
#	pairs() gives correlation matrix and plots; test on a random sample from our data
df_sample <- sample_n(df,10000,replace=FALSE) 
df_sample %>% select(age,policy_year,sum_assured) %>% pairs
# or cor() to return just the correlation matrix
cor <-df_sample %>% select(age,policy_year,sum_assured) %>% cor
cor
# corrplot() is an alternative to visualise a correlation matrix
corrplot(cor)

#ggpairs() similarly shos correlations for ordered numeric data as well as other summary stats
df_sample %>% select(age,policy_year,sum_assured,sex, smoker) %>% ggpairs

# summary statistics for subsets of data
aggregate(df$sum_assured~df$age,data=df,mean)
```

## Visualisation methods
This section sets out some simple visualisation methods using ggplot().
```{r Visualisation methods, class.source = 'fold-show'}
```

## Review claim data

```{r Review claim data, class.source = 'fold-show'}
# records, claim vs no claim. should be close to nil overlapping clams. actual claim rate is ~0.003-0.005
df %>% select(inc_count_acc,inc_count_sick) %>% table()


# use ggplot to plot inc_count_sick by age and sex; using df_sample from earlier
# clearly all of the points are going to be at 0 or 1 and will overlap at each age -> not useful.
df_sample %>% ggplot(aes(x=age,y=inc_count_sick,color=sex)) +
geom_point()
# as above but add some random noise around the points to separate them
df_sample %>% ggplot(aes(x=age,y=inc_count_sick,color=sex)) +
geom_point(position=position_jitter(height=0.1))
# as above but excluding unknown sex (as there are very few claims observed for that group) and adding a smoothing line (setting method as glm)
# because the claim rate is so low, the smoothed line is very close to zero and so not a particularly useful visualisation.
df_sample %>% filter(sex != "u") %>% ggplot(aes(x=age,y=inc_count_sick,color=sex)) +
geom_point(position=position_jitter(height=0.1)) + 
geom_smooth(method="glm", method.args = list(family = "binomial")) # or list(family = binomial(link='logit')

# looking at total count of claim rather than just sickness shows a slight trend by age
df_sample %>% filter(sex != "u") %>% ggplot(aes(x=age,y=inc_count_tot,color=sex)) +
geom_point(position=position_jitter(height=0.1)) + 
geom_smooth(method="glm", method.args = list(family = "binomial")) # or list(family = binomial(link='logit')

# given the actual count of claims is so low, it might be more useful to consider the claim rate
# use the manipulation methods from earlier to get claim rates by age and sex for accident and sickness; filter out unknown sex and age with low exposure
# this shows a clear trend by age for males and females
df %>% filter(sex != "u", between(age, 30,60)) %>% group_by(age,sex) %>% summarise(total_sick=sum(inc_count_sick),total_acc=sum(inc_count_acc), exposure=n()) %>% 
mutate(sick_rate = total_sick/exposure, acc_rate = total_acc/exposure) %>%
# used ggplot to graph the results
ggplot(aes(x=age,y=sick_rate,color=sex)) +
geom_point() +
geom_line() +
# add a smoothing line
geom_smooth(method = 'glm',se=FALSE)



```

# Model selection {.tabset .tabset-fade .tabset-pills}
The sections below provide a refresher on linear and logistic regression; some considerations for insurance data; model selection and testing model fit.

## Models for grouped vs ungrouped data

## Splitting data
Split data into training and testing data sets.

```{r Split data, class.source = 'fold-show'}
# Determine the number of rows for training
nrow(df)
# Create a random sample of row IDs
sample_rows <- sample(nrow(df),0.75*nrow(df))
# Create the training dataset
df_train <- df[sample_rows,]
# Create the test dataset
df_test <- df[-sample_rows,]
```

## Regression background
Linear vs logistic retreggion. Cover the choice of: response distribution, link (and offsets), explanatory variables 

## Actuals or AvE?

## Stepwise regression
```{r Model selection, class.source = 'fold-show'}
# specify a null model with no predictors
null_model_sick <- glm(inc_count_sick ~ 1, data = df_train, family = "binomial")

# specify the full model using all of the potential predictors
full_model_sick <- glm(inc_count_sick ~ cal_year + policy_year + sex + smoker + benefit_period + waiting_period + occupation + age + sum_assured, data = df_train, family = "binomial")

# use a forward stepwise algorithm to build a parsimonious model
step_model_sick <- step(null_model_sick, scope = list(lower = null_model_sick, upper = full_model_sick), direction = "forward")

summary(full_model_sick)
summary(step_model_sick)

```

# Evaluation {.tabset .tabset-fade .tabset-pills}
## Techniques
R$^{2}$
etc.

## Residual plots

## Confusion matrix

## ROC/ AUC
ROC (Receiver Operating Characteristic) curve

## Out of sample predictions

# References {.tabset .tabset-fade .tabset-pills}
[R Markdown Cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/)
[Business mix - Actuaries Institute Preso](http://www.actuaries.org/HongKong2012/Presentations/WBR5_Louw.pdf)
[Tidyverse documentation](https://dplyr.tidyverse.org/index.html)
